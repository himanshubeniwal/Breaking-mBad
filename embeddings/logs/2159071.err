Traceback (most recent call last):
  File "/home/khv4ky/toxicity/zeroshot_parallel_detox/embeddings/2-aya-8b.py", line 478, in <module>
    main()
  File "/home/khv4ky/toxicity/zeroshot_parallel_detox/embeddings/2-aya-8b.py", line 420, in main
    analyzer = TextAnalyzer(args.model_name, device=args.device)
  File "/home/khv4ky/toxicity/zeroshot_parallel_detox/embeddings/2-aya-8b.py", line 25, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "/home/khv4ky/.conda/envs/EasyEdit/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py", line 953, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/khv4ky/.conda/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2020, in from_pretrained
    raise EnvironmentError(
OSError: Can't load tokenizer for '/scratch/khv4ky/models_backup_2/models/bloom/trained_model/checkpoints/checkpoint-1260'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/scratch/khv4ky/models_backup_2/models/bloom/trained_model/checkpoints/checkpoint-1260' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.
