# Breaking-mBad

As large language models (LLMs) become increasingly prevalent in global applications, ensuring they are toxicity-free across diverse linguistic contexts remains a critical challenge. We present _Breaking mBad_, a cross-lingual paradigm that mitigates toxicity across multiple languages through a "capture-and-transfer" methodology, enabling robust detoxification capabilities to transfer between high and low-resource languages across Indo-European and Non-Indo-European families. We analyze cross-lingual detoxification's effectiveness through 504 extensive experiments to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation.

